

#  Language Detector


## Project Overview

This project provides a modular framework for identifying the language of a given text using different machine learning approaches:

- **Character-level N-Gram + Multinomial Naive Bayes**
- **RNN (LSTM) model trained from scratch**

The system supports command-line interface (CLI), making it easy to test different models without modifying source code.

---

##  Requirements

Make sure you have Python 3.8+ installed.  



```bash
pip install -r requirements.txt
```

---

## Project Structure

```
language-detector/
│
├── lang_detector/             # Core package
│   ├── detectors/             # Different detector implementations
│   │   ├── base.py            # Abstract base class
│   │   ├── ngram.py           # N-Gram + MultinomialNB
│   │   └── rnn.py             # RNN (LSTM) model
│   ├── data/                  # Data loading and preprocessing
│   │   ├── loader.py          # Load Tatoeba dataset
│   │   └── preprocess.py      # Text preprocessing utilities
│   ├── schemas.py             # Pydantic models for input/output validation
│   ├── utils.py               # Helper functions (e.g., dataset splitting)
│   └── __init__.py
│
├── data/                      # Raw data files
│   └── sentences.csv          # Tatoeba dataset file
├── tests/                     # test files
│   └── test_ngram.py
├── main.py                    # CLI entry point for training & testing
├── README.md                  # This file
└── requirements.txt           # Dependency list
```


## Getting Started

### Step 1: Prepare Dataset

Download the [Tatoeba dataset](https://tatoeba.org/en/downloads) and place `sentences.csv` in the `data/` folder.

### Step 2: Run the Program

You can run the program with different language detectors via command line arguments.

#### Use N-Gram detector:
```bash
python main.py --detector ngram --samples-per-lang 1000
```

#### Use RNN detector:
```bash
python main.py --detector rnn --samples-per-lang 1000
```

---

##  Dataset Description

### Source
- **Dataset**: [Tatoeba Sentence Corpus](https://tatoeba.org/en/downloads)


---

##  Models Implemented


### 1. N-Gram Language Detector

**Algorithm Description**:

This detector uses character-level n-grams as features and applies a multinomial Naive Bayes classifier for language classification.

- **Feature Extraction**: Text is converted into character-based n-grams (e.g., bigrams, trigrams) using `CountVectorizer` with `analyzer='char'` and `ngram_range=(2, 4)`.
- **Classification**: A `MultinomialNB` model from `scikit-learn` is trained on the vectorized features to predict the language.
- **Prediction Output**: Returns the predicted language label and confidence based on probability distribution.

### 2. RNN Language Detector (LSTM)

**Algorithm Description**:

This detector uses a character-level LSTM network to model language sequences.

- **Input Representation**: Each character is mapped to an integer index. Input sequences are padded or truncated to a fixed length (`max_len=100`).
- **Model Architecture**:
  - Embedding Layer: Maps character indices to dense vectors of size 64.
  - LSTM Layer: Processes the sequence and captures dependencies between characters.
  - Fully Connected Layer: Outputs logits corresponding to each language class.
- **Training Parameters**:
  - `num_epochs`: Number of training epochs (default: 10)
  - `batch_size`: Size of mini-batches (default: 64)
  - `embedding_dim`: Dimension of character embeddings (64)
  - `hidden_dim`: Size of LSTM hidden layer (128)
  - `learning_rate`: Optimizer learning rate (default: 0.001)

---

## Sample Output

```
[INFO] Loading data with 1000 samples per language...
[INFO] Using detector: NGramLanguageDetector
[INFO] Training model...
[INFO] Evaluating model...
[ERROR] index 427
sentence: Tom ging hinaus.
label: deu
pred: eng
--------------------------------------------------
[ERROR] index 499
sentence: Iré a Detroit.
label: spa
pred: fra
--------------------------------------------------
[ERROR] index 699
sentence: Yuba a appelé Mary.
label: fra
pred: spa
--------------------------------------------------
Accuracy: 99.70%

Total sentences: 5000
Language distribution:
fra: 1000 (20.00%)
spa: 1000 (20.00%)
eng: 1000 (20.00%)
deu: 1000 (20.00%)
rus: 1000 (20.00%)

```

---

##  References

- Cavnar W B, Trenkle J M. N-gram-based text categorization[C]//Proceedings of SDAIR-94, 3rd annual symposium on document analysis and information retrieval. 1994, 161175: 14.

PS. This ReadMe is generated by LLM